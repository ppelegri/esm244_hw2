---
title: 'Task 1: Palmetto Binary Logistic Regression (PELEGRI)'
author: "Patrick Pelegri-O'Day"
date: "1/30/2022"
output:
  html_document: 
    theme: flatly
    highlight: pygments
code_folding: hide
---

```{r setup, include=TRUE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(tidyverse)
library(GGally)
library(here)
library(cowplot)
library(broom)
library(jtools)
library(caret)
library(kableExtra)
```


A. Overview 
B. 3 finalized plots exploring differences in variables for the two species
C. Run binary logistic regression
  - Then perform cross validation
  - Then check AICc
D. Train selected model on entire dataset and show results in a table
E. Show prediction accuracy in a table


### Overview
This report uses binary logistic regression to test the feasibility of using variables plant height, canopy length, canopy width, and number of green leaves to classify whether a palmetto is species Serenoa repens or Sabal etonia. The dataset is drawn from observations of two dominant palmetto species of south-central Florida from 1981 - 2017.

**Data source:** Abrahamson, W.G. 2019. Survival, growth and biomass estimates of two dominant palmetto species of south-central Florida from 1981 - 2017, ongoing at 5-year intervals ver 1. Environmental Data Initiative. https://doi.org/10.6073/pasta/f2f96ec76fbbd4b9db431c79a770c4d5

### Data exploration

A subset of the data is retained for analysis. The variables considered include species of palmetto, plant height, canopy length, canopy width, and green leaves.
```{r read and wrangle data}
# Read in the data
palmetto_raw <- read_csv(here("data", "palmetto.csv"))

# Take subset for exploration and analysis. Rename species variables by their latin name and convert to factor
palmetto <- palmetto_raw %>% 
  select(species, height, length, width, green_lvs) %>% 
  mutate(species = case_when(
    species == 1 ~ "Serenoa repens",
    species == 2 ~ "Sabal etonia"
  )) %>% 
  mutate(species = as.factor(species)) %>%  # Sabal etonia is 1st, Serenoa repens is 2nd
  drop_na()
  
```

Below, differences in plant height, canopy length, canopy width, and green leaves are explored for the two species, Serenoa repens and Sabal etonia.

```{r data exploration}
# Explore differences between the two species

# The code below explores the four variables of interest with one plot for each variable. The code for each plot is the same except for the variable name.
explore1 <-ggplot(data = palmetto, aes(x = species, y = green_lvs, fill = species)) + 
  geom_violin(scale = "count", color = "black") + # mirrored density plot
geom_boxplot(color = "black", fill = NA, width = 0.1, outlier.color = NA) + # add  median and quartiles
  stat_summary(fun=mean, # add the mean to the figure 
               geom="point", 
               shape=20, 
               size=4, 
               color="black", 
               fill="black") +
  scale_fill_manual(values = c("cornsilk4", "lightblue3")) + 
  theme_minimal(13) + # change theme and font size
  theme(legend.position = "none",
        axis.text.x = element_text(vjust = 5, size = 12)) + 
  labs(x = element_blank(), y = "# of green leaves")

explore2 <-ggplot(data = palmetto, aes(x = species, y = height, fill = species)) + 
  geom_violin(scale = "count", color = "black") + 
geom_boxplot(color = "black", fill = NA, width = 0.1, outlier.color = NA) + 
  stat_summary(fun=mean,
               geom="point", 
               shape=20, 
               size=4, 
               color="black", 
               fill="black") +
  scale_fill_manual(values = c("cornsilk4", "lightblue3")) + 
  theme_minimal(13) + 
  theme(legend.position = "none",
        axis.text.x = element_text(vjust = 5, size = 12)) + 
  labs(x = element_blank(), y = "Plant height")

explore3 <-ggplot(data = palmetto, aes(x = species, y = length, fill = species)) + 
  geom_violin(scale = "count", color = "black") + 
geom_boxplot(color = "black", fill = NA, width = 0.1, outlier.color = NA) + 
  stat_summary(fun=mean,
               geom="point", 
               shape=20, 
               size=4, 
               color="black", 
               fill="black") +
  scale_fill_manual(values = c("cornsilk4", "lightblue3")) + 
  theme_minimal(13) + 
  theme(legend.position = "none",
        axis.text.x = element_text(vjust = 5, size = 12)) + 
  labs(x = element_blank(), y = "Canopy length")


explore4 <-ggplot(data = palmetto, aes(x = species, y = width, fill = species)) + 
  geom_violin(scale = "count", color = "black") + 
geom_boxplot(color = "black", fill = NA, width = 0.1, outlier.color = NA) + 
  stat_summary(fun=mean,
               geom="point", 
               shape=20, 
               size=4, 
               color="black", 
               fill="black") +
  scale_fill_manual(values = c("cornsilk4", "lightblue3")) + 
  theme_minimal(13) + 
  theme(legend.position = "none",
        axis.text.x = element_text(vjust = 5, size = 12)) + 
  labs(x = element_blank(), y = "Canopy width")

plot_grid(explore1, explore2, explore3, explore4, labels = c('A', 'B', 'C', 'D'), label_size = 12, nrow = 2)
```

We see that the two species are difficult to distinguish when looking at canopy length, canopy width, and canopy height. However the distribution of number of green leaves is noticeably different between the two species. Given that green leaves is the only variable that has a different trend between the two species, that is the variable we will focus on for binomial logistic regression.

### Binary logistic regression

Two binary logistic regression models are created to predict palmetto species based on palmetto traits. One model contains all four variables of interest and is called **Model All**. The other model contains only canopy width, plant height, and number of green leaves and is called **Model Subset**.

```{r binary logistic regression}

# Define two functions that will be the basis of two models
f_all <- species ~ green_lvs + width + height + length
f_subset <- species ~ green_lvs + width + height 

# Define two binary logistic regression models based on the functions above
palmetto_blr_all <- glm(formula = f_all,
                    data = palmetto,
                    family = 'binomial')

palmetto_blr_subset <- glm(formula = f_subset,
                    data = palmetto,
                    family = 'binomial')

```

Cross validation using 10 folds and 10 iterations is performed to compare the performance of the two models. 
```{r cross validation}

set.seed(47)

# Define training method as cross validation, 10 folds, 10 repeats
tr_ctrl <- trainControl(method = 'repeatedcv', number = 10, repeats = 10)

# Train the two models
model_all <- train(f_all, data = palmetto,
                method = 'glm', family = 'binomial',
                trControl = tr_ctrl)

model_subset <- train(f_subset, data = palmetto,
                method = 'glm', family = 'binomial',
                trControl = tr_ctrl)

```

Cross-validation results show that Model All (accuracy of `r round(model_all$results$Accuracy, 1)`) outperforms Model Subset (AICc of `r round(model_subset$results$Accuracy, 1)`) by approximately ``r round((model_all$results$Accuracy - model_subset$results$Accuracy), 1)` points. Thus, cross-validation indicates that Model All is preferable.

```{r AIC}
# Also compare AICc
palmetto_aic <- AICcmodavg::aictab(list(palmetto_blr_all, palmetto_blr_subset))

palmetto_aic # AIC results indicate that Model 1 is preferable since it is lower by 793 points
```

AICc results show that Model All (AICc of `r round(palmetto_AICc$AICc[1], 1)`) outperforms Model Subset (AICc of `r round(palmetto_AICc$AICc[2], 1)`) by approximately `r round(palmetto_AICc$Delta_AICc[2], 1)` points. This difference is very large - much larger than the significant difference threshold of 2 points. AICc confirms that Model All is preferable to Model Subset.

```{r train selected model}
# Store tidy results of final model (Model 1)
final_palmetto_blr <- tidy(palmetto_blr1) %>%  
  mutate(p.value = case_when( # rename p-values for readability
    p.value <0.001 ~ "<0.001"
  )) %>% 
  mutate(term = case_when( 
         term == "(Intercept)" ~ "Intercept",
         term == "height" ~ "Height",
         term == "length" ~ "Length",
         term == "width" ~ "Width",
         term == "green_lvs" ~ "# of green leaves")) %>% 
  select(-statistic)

# Create table
kable(final_palmetto_blr, 
      col.names = c("Term", "Coefficient", "Standard Error", "P Value")) %>% 
  kable_minimal(full_width = FALSE)
```

```{r}
# Put the below as text if I think it's necessary.

# Final model:
# `r equatiomatic::extract_eq(final_palmetto_blr, wrap = TRUE, use_coefs = TRUE)`
```


#### Evaluate model predictions

```{r evaluate model predictions}
# Create function for prediction accuracy
predict_accuracy_f <- function(x, y) {
  accurate <- ifelse(x == y, 1, 0)
  return(accurate)
}
 
# Create fitted outcomes for each model, create column predicted_species based on predictions, create column prediction_result that shows 1 for correct prediction and 0 for incorrect prediction
blr1_predictions <- palmetto_blr1 %>% 
  augment(type.predict = 'response') %>% 
  mutate(predicted_species = case_when(
    .fitted >= 0.5 ~ "Serenoa repens",
    .fitted < 0.5 ~ "Sabal etonia"
  )) %>% 
  mutate(prediction_result = predict_accuracy_f(species, predicted_species))

# See what percentage of the time the model predicted correctly
blr1_accuracy <- blr1_predictions %>% 
  group_by(species) %>% 
  summarize(pred_accuracy_v = round(mean(prediction_result), 2))
  
```

**Table 2.** Table caption here
```{r}
# Return results in a table
kable(blr1_accuracy, 
      col.names = c("Species", "% correctly classified")) %>% 
  kable_minimal(full_width = FALSE)
```
